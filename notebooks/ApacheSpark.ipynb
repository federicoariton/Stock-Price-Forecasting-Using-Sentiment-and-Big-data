{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ac3b510-8b65-457f-acbe-b8869eb598b6",
   "metadata": {},
   "source": [
    "# Federico Ariton\n",
    "# Master of Science in Data Analytics\n",
    "# Semester 2 - CA2 Integreated\n",
    "# Student Number: sba22090\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e344ff09-40c9-4c34-8f44-60e55df2ea2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 41 items\n",
      "-rw-r--r--   1 hduser supergroup      27248 2025-05-04 12:25 /user/hduser/ca2-data/stockprice/^GSPC.csv\n",
      "-rw-r--r--   1 hduser supergroup      27598 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/AAPL.csv\n",
      "-rw-r--r--   1 hduser supergroup       1475 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/ABNB.csv\n",
      "-rw-r--r--   1 hduser supergroup      27440 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/AMT.csv\n",
      "-rw-r--r--   1 hduser supergroup      27756 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/AMZN.csv\n",
      "-rw-r--r--   1 hduser supergroup      27146 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/BA.csv\n",
      "-rw-r--r--   1 hduser supergroup      27341 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/BABA.csv\n",
      "-rw-r--r--   1 hduser supergroup      28475 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/BAC.csv\n",
      "-rw-r--r--   1 hduser supergroup      25665 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/BKNG.csv\n",
      "-rw-r--r--   1 hduser supergroup      15550 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/BRK-A.csv\n",
      "-rw-r--r--   1 hduser supergroup      27910 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/BRK-B.csv\n",
      "-rw-r--r--   1 hduser supergroup      28037 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/CCL.csv\n",
      "-rw-r--r--   1 hduser supergroup      27300 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/CVX.csv\n",
      "-rw-r--r--   1 hduser supergroup      28041 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/DIS.csv\n",
      "-rw-r--r--   1 hduser supergroup      27475 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/FB.csv\n",
      "-rw-r--r--   1 hduser supergroup      27640 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/GOOG.csv\n",
      "-rw-r--r--   1 hduser supergroup      27478 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/GOOGL.csv\n",
      "-rw-r--r--   1 hduser supergroup      27542 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/HD.csv\n",
      "-rw-r--r--   1 hduser supergroup      27755 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/JNJ.csv\n",
      "-rw-r--r--   1 hduser supergroup      27724 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/JPM.csv\n",
      "-rw-r--r--   1 hduser supergroup      27736 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/KO.csv\n",
      "-rw-r--r--   1 hduser supergroup      27836 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/LOW.csv\n",
      "-rw-r--r--   1 hduser supergroup      26949 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/MA.csv\n",
      "-rw-r--r--   1 hduser supergroup      27679 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/MCD.csv\n",
      "-rw-r--r--   1 hduser supergroup      27475 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/META.csv\n",
      "-rw-r--r--   1 hduser supergroup      27970 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/MSFT.csv\n",
      "-rw-r--r--   1 hduser supergroup      26316 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/NFLX.csv\n",
      "-rw-r--r--   1 hduser supergroup      27229 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/NKE.csv\n",
      "-rw-r--r--   1 hduser supergroup      27140 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/NVDA.csv\n",
      "-rw-r--r--   1 hduser supergroup      28553 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/PFE.csv\n",
      "-rw-r--r--   1 hduser supergroup      28088 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/PG.csv\n",
      "-rw-r--r--   1 hduser supergroup      27651 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/PYPL.csv\n",
      "-rw-r--r--   1 hduser supergroup      27139 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/SBUX.csv\n",
      "-rw-r--r--   1 hduser supergroup      27517 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/TM.csv\n",
      "-rw-r--r--   1 hduser supergroup      28347 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/TSLA.csv\n",
      "-rw-r--r--   1 hduser supergroup      27357 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/TSM.csv\n",
      "-rw-r--r--   1 hduser supergroup      27009 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/UNH.csv\n",
      "-rw-r--r--   1 hduser supergroup      27579 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/UPS.csv\n",
      "-rw-r--r--   1 hduser supergroup      27650 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/V.csv\n",
      "-rw-r--r--   1 hduser supergroup      28120 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/WMT.csv\n",
      "-rw-r--r--   1 hduser supergroup      27924 2025-05-04 12:22 /user/hduser/ca2-data/stockprice/XOM.csv\n",
      "Found 1 items\n",
      "-rw-r--r--   1 hduser supergroup    1126042 2025-05-04 12:22 /user/hduser/ca2-data/stocktweet/stocktweet.csv\n"
     ]
    }
   ],
   "source": [
    "# Check inside stockprice\n",
    "!hdfs dfs -ls /user/hduser/ca2-data/stockprice\n",
    "\n",
    "# Check inside stocktweet\n",
    "!hdfs dfs -ls /user/hduser/ca2-data/stocktweet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d38e4438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.3/624.3 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nltk>=3.9\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 KB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk>=3.9->textblob) (8.0.3)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-1.5.0-py3-none-any.whl (307 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 KB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, regex, joblib, nltk, textblob\n",
      "Successfully installed joblib-1.5.0 nltk-3.9.1 regex-2024.11.6 textblob-0.19.0 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dca4316e-831b-4b15-8aff-7c99b9d4d833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, lower, regexp_replace, avg, count, lit, when, lag, stddev\n",
    "from pyspark.sql.types import FloatType\n",
    "from textblob import TextBlob\n",
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249f49fe-2b7d-4a8b-a56e-a251031dfe65",
   "metadata": {},
   "source": [
    "## Spark Session and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "355875e8-a1f4-4d13-a84c-b0dc2935463b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SentimentAnalysisWithSparkNLP\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.2.3\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4142110-48a3-40cd-a2be-5a3551afabfd",
   "metadata": {},
   "source": [
    "## Load annd Preprocess Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02c46aab-d6e8-4eb3-ba78-5d1836ee5083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- ticker: string (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      "\n",
      "+------+----------+------+--------------------+\n",
      "|    id|      date|ticker|               tweet|\n",
      "+------+----------+------+--------------------+\n",
      "|100001|01/01/2020|  AMZN|$AMZN Dow futures...|\n",
      "|100002|01/01/2020|  TSLA|$TSLA Daddy's dri...|\n",
      "|100003|01/01/2020|  AAPL|$AAPL We’ll been ...|\n",
      "|100004|01/01/2020|  TSLA|$TSLA happy new y...|\n",
      "|100005|01/01/2020|  TSLA|\"$TSLA haha just ...|\n",
      "+------+----------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load tweets CSV\n",
    "tweets_df = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"hdfs:///user/hduser/ca2-data/stocktweet/stocktweet.csv\")\n",
    "\n",
    "# Show schema and sample\n",
    "tweets_df.printSchema()\n",
    "tweets_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd649f37-8d91-4780-a8bc-e02f765d3c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+\n",
      "|ticker|date      |tweet                                                                                                                                      |tweet_clean                                                                                                               |\n",
      "+------+----------+-------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+\n",
      "|amzn  |2020-01-01|$AMZN Dow futures up by 100 points already 🥳                                                                                              |amzn dow futures up by  points already                                                                                    |\n",
      "|tsla  |2020-01-01|$TSLA Daddy's drinkin' eArly tonight! Here's to a PT of ohhhhh $1000 in 2020! 🍻                                                           |tsla daddys drinkin early tonight heres to a pt of ohhhhh  in                                                             |\n",
      "|aapl  |2020-01-01|$AAPL We’ll been riding since last December from $172.12 what to do. Decisions decisions hmm 🤔. I have 20 mins to decide. Any suggestions?|aapl well been riding since last december from  what to do decisions decisions hmm  i have  mins to decide any suggestions|\n",
      "|tsla  |2020-01-01|$TSLA happy new year, 2020, everyone🍷🎉🙏                                                                                                 |tsla happy new year  everyone                                                                                             |\n",
      "|tsla  |2020-01-01|\"$TSLA haha just a collection of greats...\"\"Mars\"\" rofl 😈😎🌠⏫🔮💸👏💪🚀🎆🎇📣🎉🎊 *bork*\"                                                |tsla haha just a collection of greatsmars rofl  bork                                                                      |\n",
      "+------+----------+-------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lower, regexp_replace, to_date\n",
    "\n",
    "# Remove $ and lowercase ticker\n",
    "tweets_df = tweets_df.withColumn(\"ticker\", regexp_replace(lower(col(\"ticker\")), \"\\\\$\", \"\"))\n",
    "\n",
    "# Clean tweet text\n",
    "tweets_df = tweets_df.withColumn(\"tweet_clean\", regexp_replace(lower(col(\"tweet\")), \"[^a-zA-Z\\\\s]\", \"\"))\n",
    "\n",
    "# CORRECT date parsing\n",
    "tweets_df = tweets_df.withColumn(\"date\", to_date(col(\"date\"), \"dd/MM/yyyy\"))\n",
    "\n",
    "# Show clean output\n",
    "tweets_df.select(\"ticker\", \"date\", \"tweet\", \"tweet_clean\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21e668cd-d19e-4f22-ae05-d3ec4fe92bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- ticker: string (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      " |-- tweet_clean: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7371a3e2-9a41-4dd3-a034-89aeb0cd5076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+--------------------+--------------------+\n",
      "|    id|      date|ticker|               tweet|         tweet_clean|\n",
      "+------+----------+------+--------------------+--------------------+\n",
      "|100001|2020-01-01|  amzn|$AMZN Dow futures...|amzn dow futures ...|\n",
      "|100002|2020-01-01|  tsla|$TSLA Daddy's dri...|tsla daddys drink...|\n",
      "|100003|2020-01-01|  aapl|$AAPL We’ll been ...|aapl well been ri...|\n",
      "|100004|2020-01-01|  tsla|$TSLA happy new y...|tsla happy new ye...|\n",
      "|100005|2020-01-01|  tsla|\"$TSLA haha just ...|tsla haha just a ...|\n",
      "+------+----------+------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17641fc5-de14-4a85-bdc4-230eb517b84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spark-nlp in /home/hduser/.local/lib/python3.10/site-packages (6.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spark-nlp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bc50c9-ad19-48fc-8edb-ef5d58b33756",
   "metadata": {},
   "source": [
    "## Sentimental Analysis Using Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db810976-2b75-48d3-8ba2-1b591f8b87b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_vivekn download started this may take some time.\n",
      "Approximate size to download 873.6 KB\n",
      "[ / ]sentiment_vivekn download started this may take some time.\n",
      "Approximate size to download 873.6 KB\n",
      "[ — ]Download done! Loading the resource.\n",
      "[ \\ ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[ | ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:======================================================>  (77 + 2) / 80]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/usr/local/spark-3.3.4-bin-hadoop3/jars/spark-core_2.12-3.3.4.jar) to field java.util.regex.Pattern.pattern\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|tweet                                                                                                                                      |result    |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|$AMZN Dow futures up by 100 points already 🥳                                                                                              |[negative]|\n",
      "|$TSLA Daddy's drinkin' eArly tonight! Here's to a PT of ohhhhh $1000 in 2020! 🍻                                                           |[negative]|\n",
      "|$AAPL We’ll been riding since last December from $172.12 what to do. Decisions decisions hmm 🤔. I have 20 mins to decide. Any suggestions?|[positive]|\n",
      "|$TSLA happy new year, 2020, everyone🍷🎉🙏                                                                                                 |[negative]|\n",
      "|\"$TSLA haha just a collection of greats...\"\"Mars\"\" rofl 😈😎🌠⏫🔮💸👏💪🚀🎆🎇📣🎉🎊 *bork*\"                                                |[negative]|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer, ViveknSentimentModel\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Step 1: Convert to document\n",
    "document = DocumentAssembler() \\\n",
    "    .setInputCol(\"tweet_clean\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "# Step 2: Tokenize\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "# Step 3: Apply pretrained sentiment model\n",
    "sentiment = ViveknSentimentModel.pretrained() \\\n",
    "    .setInputCols([\"document\", \"token\"]) \\\n",
    "    .setOutputCol(\"sentiment\")\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(stages=[document, tokenizer, sentiment])\n",
    "\n",
    "# Fit and apply\n",
    "model = pipeline.fit(tweets_df)\n",
    "result = model.transform(tweets_df)\n",
    "\n",
    "# Show results\n",
    "result.select(\"tweet\", \"sentiment.result\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e45c9a1-0d29-435c-b4cb-e41372dcec7f",
   "metadata": {},
   "source": [
    "## Extract Sentiment Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "349b4441-b8cd-46c6-a1f9-c62adecd5181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "|tweet                                                                                                                                      |sentiment_label|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "|$AMZN Dow futures up by 100 points already 🥳                                                                                              |negative       |\n",
      "|$TSLA Daddy's drinkin' eArly tonight! Here's to a PT of ohhhhh $1000 in 2020! 🍻                                                           |negative       |\n",
      "|$AAPL We’ll been riding since last December from $172.12 what to do. Decisions decisions hmm 🤔. I have 20 mins to decide. Any suggestions?|positive       |\n",
      "|$TSLA happy new year, 2020, everyone🍷🎉🙏                                                                                                 |negative       |\n",
      "|\"$TSLA haha just a collection of greats...\"\"Mars\"\" rofl 😈😎🌠⏫🔮💸👏💪🚀🎆🎇📣🎉🎊 *bork*\"                                                |negative       |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Extract the first element from the result array\n",
    "extract_sentiment = udf(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None, StringType())\n",
    "result = result.withColumn(\"sentiment_label\", extract_sentiment(col(\"sentiment.result\")))\n",
    "\n",
    "# Show results\n",
    "result.select(\"tweet\", \"sentiment_label\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61744c1-1c85-41d9-b809-a4759712c346",
   "metadata": {},
   "source": [
    "## Aggregate Sentiment Scores per Ticker-Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7fb6e93-fc8a-4cff-898b-b9e05905ff83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------+----+---------------+-----+\n",
      "|ticker                                                   |date|sentiment_label|count|\n",
      "+---------------------------------------------------------+----+---------------+-----+\n",
      "| 116.50????? 😍🙏🏼\"                                     |null|null           |1    |\n",
      "| and obviously sales hit!                                |null|null           |1    |\n",
      "| nissan leaf                                             |null|negative       |1    |\n",
      "| lol ! 🖕\"                                               |null|null           |1    |\n",
      "| will drop down to 2500 📉📉📉 then it will rise to 4000\"|null|null           |1    |\n",
      "|null                                                     |null|null           |1119 |\n",
      "| still would be fine. it’s tesla. 💥\"                    |null|null           |1    |\n",
      "| overvalued                                              |null|positive       |1    |\n",
      "| up a lot today🚀🚀 🚀\"                                  |null|null           |1    |\n",
      "| plus lawsuits and higher taxes                          |null|negative       |1    |\n",
      "+---------------------------------------------------------+----+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "agg_df = result.groupBy(\"ticker\", \"date\", \"sentiment_label\").count().orderBy(\"date\")\n",
    "agg_df.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9438eef-232e-461e-9d5d-4a577f5cbf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+--------+-------+\n",
      "|ticker|      date|positive|negative|neutral|\n",
      "+------+----------+--------+--------+-------+\n",
      "|   bac|2020-07-16|       0|       1|      0|\n",
      "|   ccl|2020-11-13|       1|       1|      0|\n",
      "|  amzn|2020-08-05|       0|       2|      0|\n",
      "|    ba|2020-04-15|       4|       5|      0|\n",
      "|    ba|2020-12-22|       2|       0|      0|\n",
      "|  tsla|2020-11-11|       1|       2|      0|\n",
      "|  nvda|2020-12-01|       1|       0|      0|\n",
      "|  tsla|2020-01-13|       8|       1|      0|\n",
      "|    ba|2020-03-25|      20|      20|      0|\n",
      "|  tsla|2020-01-24|       5|       2|      0|\n",
      "+------+----------+--------+--------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pivot_df = agg_df.groupBy(\"ticker\", \"date\") \\\n",
    "    .pivot(\"sentiment_label\", [\"positive\", \"negative\", \"neutral\"]) \\\n",
    "    .sum(\"count\") \\\n",
    "    .fillna(0)\n",
    "\n",
    "pivot_df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ba43364-6825-4d6e-b696-0efd26eec220",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 20:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+--------+-------+\n",
      "|ticker|      date|positive|negative|neutral|\n",
      "+------+----------+--------+--------+-------+\n",
      "|   bac|2020-07-16|       0|       1|      0|\n",
      "|   ccl|2020-11-13|       1|       1|      0|\n",
      "|  amzn|2020-08-05|       0|       2|      0|\n",
      "|    ba|2020-04-15|       4|       5|      0|\n",
      "|    ba|2020-12-22|       2|       0|      0|\n",
      "|  tsla|2020-11-11|       1|       2|      0|\n",
      "|  nvda|2020-12-01|       1|       0|      0|\n",
      "|  tsla|2020-01-13|       8|       1|      0|\n",
      "|    ba|2020-03-25|      20|      20|      0|\n",
      "|  tsla|2020-01-24|       5|       2|      0|\n",
      "|  sbux|2020-10-08|       0|       1|      0|\n",
      "|  baba|2020-12-03|       1|       0|      0|\n",
      "|    ma|2020-03-05|       0|       1|      0|\n",
      "|  tsla|2020-01-29|       7|      10|      0|\n",
      "|  tsla|2020-07-14|      14|      15|      0|\n",
      "|    fb|2020-07-07|       0|       2|      0|\n",
      "|    fb|2020-03-05|       1|       3|      0|\n",
      "|  baba|2020-02-12|       1|       1|      0|\n",
      "|     v|2020-03-18|       1|       0|      0|\n",
      "|   nke|2020-02-03|       0|       1|      0|\n",
      "|  msft|2020-03-03|       0|       2|      0|\n",
      "|  msft|2020-03-18|       0|       1|      0|\n",
      "|  amzn|2020-04-30|       2|       4|      0|\n",
      "|  aapl|2020-11-04|       3|       3|      0|\n",
      "|  aapl|2020-03-26|       3|       5|      0|\n",
      "|  nflx|2020-11-11|       1|       0|      0|\n",
      "|  baba|2020-12-22|       1|       3|      0|\n",
      "|    ba|2020-05-27|       2|       0|      0|\n",
      "|  aapl|2020-08-01|       2|       3|      0|\n",
      "|  aapl|2020-03-08|       0|       1|      0|\n",
      "|  goog|2020-10-20|       1|       0|      0|\n",
      "|   jnj|2020-12-04|       1|       0|      0|\n",
      "|  amzn|2020-04-22|       1|       0|      0|\n",
      "|  amzn|2020-10-06|       2|       0|      0|\n",
      "|  nflx|2020-04-13|       1|       0|      0|\n",
      "|   mcd|2020-10-24|       0|       1|      0|\n",
      "|  aapl|2020-02-19|       2|       1|      0|\n",
      "|  baba|2020-08-19|       1|       2|      0|\n",
      "|    fb|2020-02-12|       1|       0|      0|\n",
      "|   wmt|2020-07-14|       0|       1|      0|\n",
      "|  pypl|2020-10-21|       0|       0|      0|\n",
      "|    fb|2020-06-27|       3|       0|      0|\n",
      "|  amzn|2020-12-02|       1|       1|      0|\n",
      "|  baba|2020-11-12|       0|       2|      0|\n",
      "|  baba|2020-07-14|       0|       1|      0|\n",
      "|  amzn|2020-03-04|       0|       1|      0|\n",
      "|  tsla|2020-10-01|       8|      13|      0|\n",
      "|  abnb|2020-12-18|       1|       0|      0|\n",
      "|  aapl|2020-10-04|       2|       1|      0|\n",
      "|   pfe|2020-07-29|       1|       1|      0|\n",
      "|   dis|2020-05-09|       0|       2|      0|\n",
      "|   ccl|2020-04-07|       0|       4|      0|\n",
      "|  nflx|2020-02-04|       0|       2|      0|\n",
      "|  aapl|2020-11-29|       0|       1|      0|\n",
      "|  baba|2020-03-27|       1|       0|      0|\n",
      "|  aapl|2020-02-03|       1|       2|      0|\n",
      "|  baba|2020-03-24|       1|       0|      0|\n",
      "|   wmt|2020-10-13|       0|       1|      0|\n",
      "|  tsla|2020-11-06|       1|       4|      0|\n",
      "|   pfe|2020-10-21|       0|       1|      0|\n",
      "|    hd|2020-06-03|       1|       0|      0|\n",
      "|  tsla|2020-04-23|       0|       2|      0|\n",
      "|   nke|2020-08-19|       1|       0|      0|\n",
      "|  tsla|2020-01-18|       1|       4|      0|\n",
      "|  sbux|2020-02-13|       1|       0|      0|\n",
      "|  tsla|2020-10-28|       1|       4|      0|\n",
      "|   ups|2020-07-30|       2|       1|      0|\n",
      "|   ccl|2020-12-31|       0|       1|      0|\n",
      "|    ba|2020-03-12|       3|       5|      0|\n",
      "|  aapl|2020-01-23|       0|       3|      0|\n",
      "|  aapl|2020-10-12|       8|       9|      0|\n",
      "|  tsla|2020-08-27|      10|      16|      0|\n",
      "|  tsla|2020-10-09|       4|       7|      0|\n",
      "|  tsla|2020-02-22|       1|       1|      0|\n",
      "|    ba|2020-12-09|       0|       1|      0|\n",
      "|  amzn|2020-01-07|       1|       0|      0|\n",
      "|  baba|2020-12-25|       0|       0|      0|\n",
      "|  msft|2020-02-14|       1|       0|      0|\n",
      "|  pypl|2020-07-29|       0|       1|      0|\n",
      "|   pfe|2020-11-20|       1|       1|      0|\n",
      "|    fb|2020-06-29|       1|       0|      0|\n",
      "|    ba|2020-04-22|       0|       1|      0|\n",
      "|    ba|2020-03-11|       1|       6|      0|\n",
      "|   dis|2020-10-14|       1|       0|      0|\n",
      "|  tsla|2020-01-17|       9|       4|      0|\n",
      "|  nflx|2020-04-20|       1|       0|      0|\n",
      "|   ccl|2020-09-04|       0|       2|      0|\n",
      "|  msft|2020-10-24|       1|       0|      0|\n",
      "|  aapl|2020-06-19|       6|       1|      0|\n",
      "|  amzn|2020-09-22|       3|       3|      0|\n",
      "|   pfe|2020-04-09|       0|       1|      0|\n",
      "|     v|2020-07-11|       0|       1|      0|\n",
      "|  aapl|2020-09-18|      14|      13|      0|\n",
      "|  msft|2020-08-14|       1|       0|      0|\n",
      "|  msft|2020-07-17|       1|       2|      0|\n",
      "|   unh|2020-08-05|       1|       0|      0|\n",
      "| googl|2020-07-30|       0|       1|      0|\n",
      "|  tsla|2020-12-21|       5|       6|      0|\n",
      "|    ba|2020-07-14|       0|       2|      0|\n",
      "|  nflx|2020-10-07|       1|       0|      0|\n",
      "+------+----------+--------+--------+-------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pivot_df.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a469f73-26ae-4d9d-a817-75f8d3977f6d",
   "metadata": {},
   "source": [
    "## Calculate Average Sentiment Score and Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b51daea2-bc75-4929-b49d-d1e0f7c9e1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 26:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------------------+------------+\n",
      "|ticker|      date|      avg_sentiment|tweet_volume|\n",
      "+------+----------+-------------------+------------+\n",
      "|  tsla|2020-01-13| 0.7777777777777778|           9|\n",
      "|  tsla|2020-01-24|0.42857142857142855|           7|\n",
      "|    ba|2020-03-25|                0.0|          41|\n",
      "|    ba|2020-04-15|-0.1111111111111111|           9|\n",
      "|   bac|2020-07-16|               -1.0|           1|\n",
      "|  amzn|2020-08-05|               -1.0|           2|\n",
      "|  tsla|2020-11-11|-0.3333333333333333|           3|\n",
      "|   ccl|2020-11-13|                0.0|           2|\n",
      "|  nvda|2020-12-01|                1.0|           1|\n",
      "|    ba|2020-12-22|                1.0|           2|\n",
      "+------+----------+-------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, avg, count\n",
    "\n",
    "# Assign +1 to positive, -1 to negative, 0 to neutral\n",
    "result = result.withColumn(\n",
    "    \"sentiment_score\",\n",
    "    when(col(\"sentiment_label\") == \"positive\", 1)\n",
    "    .when(col(\"sentiment_label\") == \"negative\", -1)\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "# Aggregate avg sentiment and tweet volume per ticker-date\n",
    "avg_sentiment_df = result.groupBy(\"ticker\", \"date\").agg(\n",
    "    avg(\"sentiment_score\").alias(\"avg_sentiment\"),\n",
    "    count(\"tweet\").alias(\"tweet_volume\")\n",
    ")\n",
    "\n",
    "avg_sentiment_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2cca322-1a38-421b-8f5e-a0d50d6c7663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+--------+-------+-------------------+------------+\n",
      "|ticker|      date|positive|negative|neutral|      avg_sentiment|tweet_volume|\n",
      "+------+----------+--------+--------+-------+-------------------+------------+\n",
      "|   bac|2020-07-16|       0|       1|      0|               -1.0|           1|\n",
      "|   ccl|2020-11-13|       1|       1|      0|                0.0|           2|\n",
      "|  amzn|2020-08-05|       0|       2|      0|               -1.0|           2|\n",
      "|    ba|2020-04-15|       4|       5|      0|-0.1111111111111111|           9|\n",
      "|    ba|2020-12-22|       2|       0|      0|                1.0|           2|\n",
      "|  tsla|2020-11-11|       1|       2|      0|-0.3333333333333333|           3|\n",
      "|  nvda|2020-12-01|       1|       0|      0|                1.0|           1|\n",
      "|  tsla|2020-01-13|       8|       1|      0| 0.7777777777777778|           9|\n",
      "|    ba|2020-03-25|      20|      20|      0|                0.0|          41|\n",
      "|  tsla|2020-01-24|       5|       2|      0|0.42857142857142855|           7|\n",
      "+------+----------+--------+--------+-------+-------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_sentiment_df = pivot_df.join(avg_sentiment_df, on=[\"ticker\", \"date\"], how=\"left\")\n",
    "final_sentiment_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618fb821-520d-491e-a8f0-9a9b02b87fc0",
   "metadata": {},
   "source": [
    "## Load and Merge Stock Price Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19400116-7993-44d3-b276-1c8e1c41fd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- ticker: string (nullable = false)\n",
      "\n",
      "+-------------------+-----------------+-----------------+-----------------+-----------------+-----------------+---------+------+\n",
      "|               Date|             Open|             High|              Low|            Close|        Adj Close|   Volume|ticker|\n",
      "+-------------------+-----------------+-----------------+-----------------+-----------------+-----------------+---------+------+\n",
      "|2019-12-31 00:00:00|72.48249816894531|73.41999816894531|72.37999725341797| 73.4124984741211|71.52082061767578|100805600|  AAPL|\n",
      "|2020-01-02 00:00:00|74.05999755859375| 75.1500015258789|73.79750061035156| 75.0875015258789|73.15264892578125|135480400|  AAPL|\n",
      "|2020-01-03 00:00:00| 74.2874984741211| 75.1449966430664|           74.125|74.35749816894531|72.44145965576172|146322800|  AAPL|\n",
      "|2020-01-06 00:00:00|73.44750213623047|74.98999786376953|          73.1875|74.94999694824219| 73.0186767578125|118387200|  AAPL|\n",
      "|2020-01-07 00:00:00|74.95999908447266| 75.2249984741211|74.37000274658203|74.59750366210938|72.67527770996094|108872000|  AAPL|\n",
      "+-------------------+-----------------+-----------------+-----------------+-----------------+-----------------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "tickers = [\"AAPL\", \"TSLA\", \"AMZN\", \"DIS\", \"BA\", \"MSFT\"]\n",
    "stock_dfs = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    path = f\"hdfs:///user/hduser/ca2-data/stockprice/{ticker}.csv\"\n",
    "    df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(path)\n",
    "    df = df.withColumn(\"ticker\", lit(ticker))\n",
    "    stock_dfs.append(df)\n",
    "\n",
    "# Union all into one DataFrame\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "stock_df = reduce(DataFrame.unionByName, stock_dfs)\n",
    "\n",
    "# Show schema and sample\n",
    "stock_df.printSchema()\n",
    "stock_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cbe8415-89ec-497e-8fa1-48ff03fc4cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "\n",
    "final_sentiment_df = final_sentiment_df.withColumn(\"ticker\", upper(col(\"ticker\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30cce046-b118-4dce-8b39-89515e3ee7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# Rename and format date\n",
    "stock_df = stock_df.withColumnRenamed(\"Date\", \"date\")\n",
    "stock_df = stock_df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9b73ccd-73d4-4584-8e75-313df7d41507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join stock data with sentiment features\n",
    "merged_df = stock_df.join(final_sentiment_df, on=[\"ticker\", \"date\"], how=\"left\")\n",
    "\n",
    "# Fill NA for sentiment columns with defaults\n",
    "merged_df = merged_df.fillna({\n",
    "    \"positive\": 0,\n",
    "    \"negative\": 0,\n",
    "    \"neutral\": 0,\n",
    "    \"avg_sentiment\": 0.0,\n",
    "    \"tweet_volume\": 0\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8730dec1-867f-4d08-b3f3-0592feb3c216",
   "metadata": {},
   "source": [
    "## Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e202caf8-34d9-4f5b-aa2e-5e0d488dafc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, avg, stddev\n",
    "\n",
    "w = Window.partitionBy(\"ticker\").orderBy(\"date\")\n",
    "\n",
    "merged_df = merged_df \\\n",
    "    .withColumn(\"lag_Close_1\", lag(\"Close\", 1).over(w)) \\\n",
    "    .withColumn(\"lag_sentiment_1\", lag(\"avg_sentiment\", 1).over(w)) \\\n",
    "    .withColumn(\"avg_Close_5\", avg(\"Close\").over(w.rowsBetween(-4, 0))) \\\n",
    "    .withColumn(\"volatility_5\", stddev(\"Close\").over(w.rowsBetween(-4, 0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3dc700d9-43d4-40f0-a6cd-d2d6991cf245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----------------+-------------------+------------+--------+--------+-------+-----------------+-------------------+-----------------+------------------+\n",
      "|ticker|      date|            Close|      avg_sentiment|tweet_volume|positive|negative|neutral|      lag_Close_1|    lag_sentiment_1|      avg_Close_5|      volatility_5|\n",
      "+------+----------+-----------------+-------------------+------------+--------+--------+-------+-----------------+-------------------+-----------------+------------------+\n",
      "|  AAPL|2019-12-31| 73.4124984741211|                0.0|           0|       0|       0|      0|             null|               null| 73.4124984741211|              null|\n",
      "|  AAPL|2020-01-02| 75.0875015258789|                0.0|           8|       4|       4|      0| 73.4124984741211|                0.0|            74.25|1.1844060164061108|\n",
      "|  AAPL|2020-01-03|74.35749816894531|-0.3333333333333333|           6|       2|       4|      0| 75.0875015258789|                0.0|74.28583272298177|0.8397980459362604|\n",
      "|  AAPL|2020-01-06|74.94999694824219|                0.0|           2|       1|       1|      0|74.35749816894531|-0.3333333333333333|74.45187377929688|0.7618742469514582|\n",
      "|  AAPL|2020-01-07|74.59750366210938|-0.3333333333333333|           3|       1|       2|      0|74.94999694824219|                0.0|74.48099975585937|0.6630089657610108|\n",
      "|  AAPL|2020-01-08|75.79750061035156|-0.6666666666666666|           6|       1|       5|      0|74.59750366210938|-0.3333333333333333|74.95800018310547|0.5504897881785514|\n",
      "|  AAPL|2020-01-09|77.40750122070312|               -0.5|           4|       1|       3|      0|75.79750061035156|-0.6666666666666666|75.42200012207032| 1.236826690180404|\n",
      "|  AAPL|2020-01-10| 77.5824966430664|               -0.5|           4|       1|       3|      0|77.40750122070312|               -0.5|76.06699981689454| 1.375992262295017|\n",
      "|  AAPL|2020-01-13|79.23999786376953|               -1.0|           4|       0|       4|      0| 77.5824966430664|               -0.5|           76.925|1.7827519772414326|\n",
      "|  AAPL|2020-01-14|78.16999816894531|              -0.25|           4|       1|       2|      0|79.23999786376953|               -1.0|77.63949890136719|1.2542982387515837|\n",
      "+------+----------+-----------------+-------------------+------------+--------+--------+-------+-----------------+-------------------+-----------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_df.select(\n",
    "    \"ticker\", \"date\", \"Close\", \"avg_sentiment\", \"tweet_volume\", \n",
    "    \"positive\", \"negative\", \"neutral\", \n",
    "    \"lag_Close_1\", \"lag_sentiment_1\", \"avg_Close_5\", \"volatility_5\"\n",
    ").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d255dbe5-abb7-450e-a65f-274cd02e17ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"has_sentiment\",\n",
    "    when(col(\"tweet_volume\") > 0, 1).otherwise(0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9534b603-2d58-4f23-862b-f7260e915cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----------------+-----------------+-----------------+-----------------+-----------------+---------+--------+--------+-------+-------------------+------------+-----------------+-------------------+-----------------+------------------+-------------+\n",
      "|ticker|      date|             Open|             High|              Low|            Close|        Adj Close|   Volume|positive|negative|neutral|      avg_sentiment|tweet_volume|      lag_Close_1|    lag_sentiment_1|      avg_Close_5|      volatility_5|has_sentiment|\n",
      "+------+----------+-----------------+-----------------+-----------------+-----------------+-----------------+---------+--------+--------+-------+-------------------+------------+-----------------+-------------------+-----------------+------------------+-------------+\n",
      "|  AAPL|2019-12-31|72.48249816894531|73.41999816894531|72.37999725341797| 73.4124984741211|71.52082061767578|100805600|       0|       0|      0|                0.0|           0|             null|               null| 73.4124984741211|              null|            0|\n",
      "|  AAPL|2020-01-02|74.05999755859375| 75.1500015258789|73.79750061035156| 75.0875015258789|73.15264892578125|135480400|       4|       4|      0|                0.0|           8| 73.4124984741211|                0.0|            74.25|1.1844060164061108|            1|\n",
      "|  AAPL|2020-01-03| 74.2874984741211| 75.1449966430664|           74.125|74.35749816894531|72.44145965576172|146322800|       2|       4|      0|-0.3333333333333333|           6| 75.0875015258789|                0.0|74.28583272298177|0.8397980459362604|            1|\n",
      "|  AAPL|2020-01-06|73.44750213623047|74.98999786376953|          73.1875|74.94999694824219| 73.0186767578125|118387200|       1|       1|      0|                0.0|           2|74.35749816894531|-0.3333333333333333|74.45187377929688|0.7618742469514582|            1|\n",
      "|  AAPL|2020-01-07|74.95999908447266| 75.2249984741211|74.37000274658203|74.59750366210938|72.67527770996094|108872000|       1|       2|      0|-0.3333333333333333|           3|74.94999694824219|                0.0|74.48099975585937|0.6630089657610108|            1|\n",
      "|  AAPL|2020-01-08|74.29000091552734|76.11000061035156|74.29000091552734|75.79750061035156|73.84435272216797|132079200|       1|       5|      0|-0.6666666666666666|           6|74.59750366210938|-0.3333333333333333|74.95800018310547|0.5504897881785514|            1|\n",
      "|  AAPL|2020-01-09|76.80999755859375|77.60749816894531|76.55000305175781|77.40750122070312|75.41287994384766|170108400|       1|       3|      0|               -0.5|           4|75.79750061035156|-0.6666666666666666|75.42200012207032| 1.236826690180404|            1|\n",
      "|  AAPL|2020-01-10| 77.6500015258789| 78.1675033569336|          77.0625| 77.5824966430664|75.58334350585938|140644800|       1|       3|      0|               -0.5|           4|77.40750122070312|               -0.5|76.06699981689454| 1.375992262295017|            1|\n",
      "|  AAPL|2020-01-13|77.91000366210938|79.26750183105469| 77.7874984741211|79.23999786376953| 77.1981430053711|121532000|       0|       4|      0|               -1.0|           4| 77.5824966430664|               -0.5|           76.925|1.7827519772414326|            1|\n",
      "|  AAPL|2020-01-14|79.17500305175781|79.39250183105469| 78.0425033569336|78.16999816894531|76.15571594238281|161954400|       1|       2|      0|              -0.25|           4|79.23999786376953|               -1.0|77.63949890136719|1.2542982387515837|            1|\n",
      "+------+----------+-----------------+-----------------+-----------------+-----------------+-----------------+---------+--------+--------+-------+-------------------+------------+-----------------+-------------------+-----------------+------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "145541b5-ce9d-4f85-8dfb-07181a688379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ticker: string (nullable = false)\n",
      " |-- date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- positive: long (nullable = false)\n",
      " |-- negative: long (nullable = false)\n",
      " |-- neutral: long (nullable = false)\n",
      " |-- avg_sentiment: double (nullable = false)\n",
      " |-- tweet_volume: long (nullable = false)\n",
      " |-- lag_Close_1: double (nullable = true)\n",
      " |-- lag_sentiment_1: double (nullable = true)\n",
      " |-- avg_Close_5: double (nullable = true)\n",
      " |-- volatility_5: double (nullable = true)\n",
      " |-- has_sentiment: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660c32d5-3a81-475d-a8fc-fd9189b80805",
   "metadata": {},
   "source": [
    "## Saving Final Processed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3801d8-8413-4c8c-90d8-43860b6e0247",
   "metadata": {},
   "source": [
    "## MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a9f0101-dfa7-45b1-81aa-f3fdeec62204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "merged_df.write.mode(\"overwrite\").parquet(\"hdfs:///processed-data/stock_sentiment.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2009e7bc-7b75-4113-bee4-a277a0b5cc01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617d9866-77bb-4e1c-a38b-b590a46e0d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
